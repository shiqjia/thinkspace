---
title: RNN理解及实现
# image: /images/black_hawk.png
categories:
  - dnn
tags:
  - dnn
last_modified_at: 2019-05-08T13:57:52-17:00
---

## 背景
梳理从rnn->lstm->gru->attention的理论及代码，希望通过把知识串联起来，形成自己的知识体系。   
ps：本篇主要结合李宏毅老师的课程和自己的理解梳理

## RNN

### 为什么需要RNN？
假设有一个订票系统，可以通过我说一段话，判断我的出发地和出发时间。如果我说："我要订5月1日到北京的飞机票"，通过训练一个一般的网络模型，它可以学得北京是地点，5月1日是日期。但是如果我说"我要订5月1日离开北京的机票"，模型还是只能识别出北京是出发地，因为这个时候模型只能关注到北京是地点，不能关注到上下文"离开"。     
所以为了解决这种包含语序（时间顺序）的问题就提出了RNN.

## RNN是怎么做的？
RNN实际上就是在原来的基础上加入了时序，在t时刻同时读取输入x，和之前存储的"记忆"。    
![img]({{"images/picture/RNN/rnn-1.png" | relative_url }})

虽然都是词语 taibei，但是这两个词的上下文不同（分别为“leave”和“arrive”）,所以“记忆”ai不同，这样得到的slot是台北的概率也不同，就能解决之前提到的不能区分哪个词应该填到slot的问题。

## RNN的分类
elman network: 在t+1时刻，读取$$ x_{t+1} $$和\(h_{t}\)(隐藏层)
jordan network: 在t+1时刻，读取\(x_{t+1}\)和\(y_{t}\)（输出）  
![image](/images/picture/RNN/rnn-2.png)      
双向rnn   
![image](/images/picture/RNN/rnn-2.png)     


## LSTM

### LSTM和RNN的区别
和simpleRNN相比，LSTM多了3个门：input/ output/ forget gate，最终控制模型的输出，什么时候打开，打开多少是网络自己学习的. 由于simpleRNN只有1个输入，而LSTM有4个输入，且每个输入都使用不同的w和b，所以LSTM的参数量是simpleRNN的4倍。   
![image](/images/picture/RNN/lstm-1.png) 

详细可以看图：  
![image](/images/picture/RNN/lstm-2.png)    
g(z)，f(zi)，f(zo) ,f(zf):都是输入z,wx+b之后，经过激活函数（不同的f的w,b不同）  
f(zi)，f(zo) ,f(zf)可以看做分别控制input多少的信号，output多少的信号，forget多少的信号

举例说明如下：  
![image](/images/picture/RNN/lstm-3.png)    
1）输入3，f(zi)=1,f(zf)=1; g(z)f(zi)+cf(zf)=3\*1+0\*1=3,更新c=3,f(zo)=0，输出门关闭，输出为0  
2）输入4，f(zi)=1,f(zf)=1；g(z)f(zi)+cf(zf)=4\*1+3\*1=7，更新c=7,f(zo)=0,输出门关闭，输出为0   
3）输入2，f(zi)=0,f(zf)=1；g(z)f(zi)+cf(zf)=0\*1+7\*1=7，更新c=7,f(zo)=0,输出门关闭，输出为0   
4）输入1，f(zi)=0,f(zf)=1；g(z)f(zi)+cf(zf)=0\*1+7\*1=7，更新c=7,f(zo)=1,输出门打开，输出为7   
5）输入3，f(zi)=0,f(zf)=0；g(z)f(zi)+cf(zf)=0\*1+7\*0=7，更新c=0,f(zo)=0,输出门关闭，输出为0 

### LSTM时序的体现
 
![image](/images/picture/RNN/lstm-3.png) 
 这里的zf,zi,z,zo都是向量，向量的每个维度对应不同时刻的值，有多少个时刻，向量的维度就是多少。    
![image](/images/picture/RNN/lstm-4.png)   
 这里的ct-1的计算要依据上一个时刻的数据，而且在实际的lstm的计算中，输入部分除了xt，还有输出之前的hidden state ht和ct，将这3者拼接在一起作为输入。  
![image](/images/picture/RNN/lstm-5.png)  