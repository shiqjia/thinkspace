---
title: deep-ctr
# image: /images/black_hawk.png
categories:
  - ctr预估
    
tags:
  - lr
  - dnn
  - deepfm
  - wide&deep
last_modified_at: 2019-03-10T13:57:52-17:00
---

# Deep-CTR作业总结
梳理了lr,dnn,wide&deep,deepfm的优缺点和互相之间的关系，后面还会不断完善。


## lr
- lr是线性模型，在给定一些特征的条件下，对特征取值x给予权重w，在加上偏差bias
- 只能对特征进行线性表达，不能捕捉到特征之间的非线性关系，举例：在房价预估这样的场景下，已有房子的长度和宽度这两个特征，使用lr是学习不到房子的面积（长度*宽度）特征的
- 为了表达特征之间的关系，可以将特征进行交叉计算之后再输入lr,这里称作：lrcross

## lrcross
- 为什么做特征交叉？：
    - lr是线性模型，现有特征通过线性组合不一定能够比较准确的刻画预估目标
    - 特征组合可以加入非线性表达，相当于是构造了新的特征，增强了模型的表达力，捕捉特征之间的相关性
- 存在问题：
    - 假设特征粒度很细（类似于细到n级品类级别），如果在这种情况下进程特征交叉，交叉之后的结果几乎可以表达训练样本中的每一条数据，会产生"过拟合"的问题
    - 直接进行特征交叉会让需要学习的参数量暴增，比如特征维度为为n,交叉之后就变成 \(n^{2}\)
    - 特征a和特征b进行交叉，如果这两个特征没有共同出现过或者共现次数非常少，那么这两个特征的权重将为0或者训练不充分，而点击率预估或者推荐场景的特征矩阵往往是非常稀疏的，所以这会导致模型性能不好
    - 人工进行特征交叉工程繁琐
    - 为了解决这种问题，就出现了特征矩阵进行分解，计算特征之间的內积来表征特征相关性的算法：fm
    
## dnn
- 人工的进行特征交叉，费时费力，如果能让机器自己学习特征的深层交叉关系就好了。dnn正好可以解决这个问题：
- 将特征进行one-hot之后，再通过embedding进行向量化，可以将之前的精确匹配变成模糊查找，具有更好的扩展能力。举例：<美国，圣诞节>出现在训练中，<英国，圣诞节>没有出现过，现在要对<英国，圣诞节>进行推荐。如果是精确匹配，则无法对<英国，圣诞节>进行推荐，但是如果先经过embedding映射成稠密向量的话，美国和英国的embedding向量相似，所以也可以进行推荐

## wide&deep 
- 如果想要模型同时拥有lr（线性模型）的"记忆"能力和dnn的"拓展"能力，就有了wide&deep
- wide部分:就相当于是lr,一般根据人工先验知识，将连续特征和一些简单、明显的特征交叉输入lr,让wide侧能够记住这些规则
- deep部分：相当于是dnn,通过embedding的方式将categorical/id特征映射成稠密向量，让DNN学习到这些特征之间的深层交叉，以增强扩展能力。
- Deep侧与Wide侧共享一个embedding矩阵来映射categorical/id特征到稠密向量
- ![image]({{'images/picture/widedeep_model_fig.png'}}| relative_url)

## deepfm
- wide&deep的wide部分还是用的lr，而lr做特征交叉还是存在一些问题（过拟合，分布不均匀导致权重训练不均匀等），而fm可以有效解决这些问题。因此用fm替换lr,能够自动学习到所有二次交叉项的系数。
- deep部分和wide&deep的deep部分一样
- ![image]({{'images/picture/fm_model_fig.png' | relative_url}})

## 思考题
- LR为什么要做特征交叉,其物理意义或者业务意义是什么?请举一个例子,用自己的语言说明这种交叉的意义
    - 因为lr是线性模型，只能对已有的特征加权求和。但是有的情况下特征之间相互组合会产生一个于预测结果有更强相关性的特征。
    - 例如在房价预估这个场景下，已知房子的长度和宽度，是不能通过线性组合得到房子的面积这个特征的，而面积特征与房价又是强相关的。而通过特征交叉就能得到这个特征。
    - 所以特征交叉相当于通过特征之间的组合，增加了更多高级特征。
- FM 和 DNN 中的embedding有什么相同之处,有什么不同之处?请论证这里的FM实现和基本FM中的实现等价
    - FM中的embedding是为了获取隐向量，实现特征交叉。使用隐向量的目的是为了，减少运算参数，实现特征稀疏情况下的ctr预估。
    - DNN中的embedding主要是为了将离散特征进行稠密向量化，或者说进行特征提取。
    - 共同之处就是都进行了降维
    - 基本的FM在lr的基础上加上了特征隐向量的內积，而这里的embedding就相当于是构建了所有特征的隐向量，由于特征全部都是id/类别特征，所以可以根据id/类型直接通过embedding_lookup查找对应的隐向量，再将得到的隐向量两两相乘（做內积），和基本fm的实现是一样的
- 思考怎么实现如下正则? 如果某个id出现的次数过少或者虽然出现很多次但是这个id加或者不加对模型没有影响, 怎么将它的embedding向量全部置0? 试着在你的DNN模型中增加这种正则
    - 如果已经知道具体的id,可以声明一个矩阵MASK,大小与embedding矩阵一致，除了Id对应的行为0，其余的位置均为1。先降embedding矩阵和MASK矩阵相乘之后，再使用这个embedding矩阵，就可以将对应位置embedding向量置0
    - 如果不知道具体的id,可以考虑进行如下正则:
    ![]({{'images/picture/DIN_regularization.png'| relative_url}})
    - 使用这个正则公式能针对id出现的频率不同，动态调整其参数的正则化力度。即，出现频率较高的id，给与较小的正则化力度，反之，则加大力度。
    


    